%!TEX root = thesis.tex

\chapter{Methoden} % (fold)
\label{cha:methoden}

\section{Graphen} % (fold)
\label{sec:graphen}
Die Breitensuche auf einem 1.2 GB Beispielgraph läuft, je nach Algorithmus, in knapp einer Sekunde durch (Anhang \ref{Anhang-Messwerte-Dichte}). Wenn ein Algorithmus nur sehr kurz läuft, machen konstante Laufzeitschwankungen prozentual einen größeren Anteil aus. Wenn also beispielsweise ein Algorithmus durch das Betriebssystem, das gerade etwas anderes machen muss, für 10 ms unterbrochen wird, verfälscht das eine Messung, die nur 50 ms läuft, gravierender, als eine Messung, die mehrere Sekunden läuft. Deswegen wurde versucht, größtmögliche Beispielgraphen zu generieren. Für diese Arbeit wurden Graphen in der Größenordnung von \nobreak{100 000} Knoten generiert. Diese wiederum relativ kleine Ausdehnung wurde gewählt, da der Testmodus unter anderem beinhaltet, dass bei gleichbleibender Knotenanzahl die Dichte des Graphen variiert. Sehr dicht besiedelte Graphen mit 100 000 Knoten passen gerade noch in den Arbeitsspeicher. Würden noch größere Graphen verwendet, müsste entweder die Dichte des Graphen beschränkt werden oder das Betriebssysteme müsste Speicherseiten auslagern. Da die Zeit für die Auslagerung mit in die Laufzeitmessung eingehen würde, ist zu erwarten, dass der Algorithmus dadurch deutlich langsamer wäre, als bei Probleminstanzen, die in den Hauptspeicher passen. Die einzige Schlussfolgerung aus so einer Testreihe wäre, dass externe Algorithmen langsamer sind, als interne. 

Die asymptotische Laufzeit der Breitensuche ist O(n + m) \cite{SWB-283374373}. Das führt zu der Vermutung, dass bei gleicher (absoluter) Kantenzahl und erhöhter Knotenzahl, die Laufzeit nicht wesentlich länger ist. Ein Beispiel: BFS auf einem Graph mit 1 000 000 Knoten und durchschnittlichem Knotengrad von 100 (ergibt 100 Mio Kanten) sollte kaum länger dauern, als die BFS auf einem Graph mit 100 000 Knoten und durchschnittlichem Knotengrad von 1000 (ergibt ebenso 100 Mio Kanten). Diese Annahme ist für den parallelen Fall falsch, da die zu versendenden Datenmengen erheblich größer werden, wenn der Graph mehr Knoten hat.

Es wurde ein Tool namens graph-generator \cite{graph-generator:2009:Online} eingesetzt, um zufällige Graphen zu generieren. Um einen Graphen zu erstellen, müssen folgende Parameter übergeben werden:
\begin{enumerate}
	\item Die Anzahl an Knoten.
	\item Der minimale Ausgangsgrad jedes Knotens.
	\item Der maximale Ausgangsgrad jedes Knotens.
	\item Der Exponent der Exponentialverteiltung.
	\item Der mittlerer Knotengrad z.
\end{enumerate}
Der Ausgangsgrad der Knoten ist folgendermaßen verteilt:
$$
P(X=k) \propto (k + offset)^{-exp}
$$
Der Offset wird dabei automatisch von dem Tool derart gewählt, dass sich ein durchschnittlicher Ausgangsgrad von z ergibt. Der Exponent der Exponentialverteilung \textit{exp} wurde vor den ersten Messungen auf 5 festgelegt und danach nicht mehr verändert. Die Zahl 5 wurde nach Tests mit verschiedenen Exponenten insofern als geeignet angesehen, als dass die Wahrscheinlichkeiten für Knoten mit sehr geringem Grad nicht zu hoch ist. Mit noch größeren Exponenten hatte der graph-generator Probleme.
% section graphen (end)

\section{Testplattform} % (fold)
\label{sec:testplattform}
Als Testplattform kam ein Apple Notebook von 2011 zum Einsatz. Es hat einen Intel Core i7-2720QM \enquote{Sandy Bridge} Prozessor, der mit 2.2 Ghz getaktet wird. Es stehen 4 physikalische Kerne zur Verfügung, die jeweils Intels Hyper Threading Technologie unterstützen. Dadurch sind physikalisch 8 parallel laufende Threads möglich. Beim Vergleich von sequentiellen Algorithmen mit parallelen ist zu beachten, dass der Prozessor einen Kern auf bis zu 3.3 GHz übertakten kann, falls die anderen Kerne momentan nicht verwendet werden. Der optimal erreichbare Speedup ist demnach nicht 8.0, sondern deutlich darunter. Das Testsystem ist außerdem mit 8 GB Hauptspeicher ausgestattet, der bei einem Takt von 1333 Mhz arbeitet. Auf dem Testsystem wird als Betriebssystem Mac OS X 10.6.8 \enquote{Snow Leopard} und der X10 Compiler in der Version 2.2.3 verwendet.
% section testplattform (end)

\section{Modus} % (fold)
\label{sec:modus}
Um Ergebnisse aus je einer Algorithmus - Graph - Kombination zu erhalten, wird der gewählte Algorithmus dreimal auf dem gewählten Graphen ausgeführt und die Zeit gemessen, die die Berechnung benötigt. Die Zeit, um den Graphen in den Speicher einzulesen und die Daten auf die Places aufzuteilen, wurde nicht gemessen, da sie wenig mit dem Algorithmus oder X10 zu tun hat. Die Zeit, die benötigt wird, um das Ergebnis von den beteiligten Places zurück zum Ursprungsplace zu kopieren, wird allerdings mitgemessen.

Die X10 Laufzeitumgebung liest beim Programmstart die beiden Umgebungsvariablen X10\_NPLACES und X10\_NTHREADS, in denen steht, wie viele Places lokal auf diesem Rechner simuliert werden sollen und wie viele Threads jedem dieser Places zur Verfügung stehen sollen. Die einzelnen Places werden durch Prozesse (nicht Threads) repräsentiert, wodurch sie tatsächlich getrennte Speicherbereiche haben. Dieser Aufbau entspricht zwar nicht einem Setup, in dem jeder X10 Prozess auf einen physikalisch getrennten Computer operiert, doch auch der Kontextwechsel, der bei der Kommunikation zwischen Places auftritt, ist relativ langsam und somit eine Annäherung an realen Kommunikationsoverhead. Trotzdem sind diese Ergebnisse nicht eins zu eins auf einen Rechnerverbund zu übertragen. Das liegt zum einen daran, dass die Kommunikation zwischen physikalisch getrennten Places nochmal erheblich teurer ist, als lokale Interprozesskommunkation, zum anderen können mit einem Rechnerverbund wesentlich größere Graphen bearbeitet werden, deren Effekte auf den Algorithmus in dieser Arbeit nicht gemessen werden.

Andererseits dürfen die Ergebnisse dieser Arbeit auch nicht mit der lokalen Parallelisierung der Breitensuche auf einem einzelnen Rechner verwechselt werden. Kommunikation mittels geteiltem Speicher ist deutlich schneller, als die hier verwendete Interprozesskommunikation. Es sei hier auch nochmal darauf hingewiesen, dass pro Prozess, also pro Place, immer nur ein Thread aktiv ist.
Um die Möglichkeit der Parallelität zu messen, wurde der Algorithmus in der 1D und der 2D Zerlegung jeweils in einer Konfiguration mit 1, 2, 4, 8 und 9 Places ausgeführt. Die Konfiguration mit 9 Places wurden hinzugenommen, da so bei der 2D Zerlegung eine symmetrische 3 mal 3 Zerlegung stattfinden kann. Es wurde vermutet, dass eine quadratische Anzahl an Places besonders günstig für diesen Algorithmus sind, obwohl 9 Places mehr sind, als Rechenkerne zur Verfügung stehen. Der Vollständigkeit halber wurde auch der 1D Algorithmus mit 9 Places durchgeführt. Um wirklich vollständige Ergebnisse zu bekommen, sollte pro Graph die Breitensuche einmal von jedem Knoten aus gestartet werden.  Dieses Unterfangen würde den Zeitrahmen der Arbeit jedoch sprengen.

Der Modus, mit nur einem Thread pro Place und ohne die Nutzung von geteiltem Speicher zwischen den einzelnen Ausführungsfäden lässt neben der Übertragbarkeit auf die invasive Hardware auch Rückschluss auf Manycore - Systeme zu. Bei diesen CPUs der Zukunft sollen viele kleine Rechenkerne auf einer CPU platziert werden. Cachekohärenz mit geteiltem Speicher und sehr vielen Kernen benötigt viel Overhead. Es werden deswegen auch Ansätze ohne geteilten Speicher verfolgt. Auch wird jeder Kern nur einen Ausführungsfaden gleichzeitig bearbeiten können. Die hier vorgeschlagenen Implementierungen sind also auch für solch eine Hardware interessant.
% section modus (end)

\chapter{Ergebnisse und Diskussion} % (fold)
\label{cha:ergebnisse_und_diskussion}

Die vollständigen Messergebnisse finden sich in den Anhängen \ref{Anhang-Messwerte-Dichte}, \ref{Anhang-Messwerte-Verteilung} und \ref{Anhang-Messwerte-Groesse}. Es wurden die Auswirkungen der Variation der Dichte, der Knotenzahl und der Verteilung des Graphen auf die Laufzeit gemessen, wobei jeweils die anderen Parameter fest gewählt waren. Sehr dünn besetzte und kleine Graphen waren deutlich schneller mit einer seriellen Version der Breitensuche zu lösen, als es mit jedwedem parallelen Algorithmus möglich war. Der Vergleich der seriellen Breitensuche, mit der 1D-Breitensuche wird in Kapitel \ref{sec:serieller_fall_vs_1d_mit_einem_place} vertieft. Der 2D-Algorithmus war in allen Testfällen der langsamste, weswegen er gesondert in Kapitel \ref{sec:die_2d_breitensuche} behandelt wird.

\section{Serieller Fall, 1D mit einem Place und 2D mit einem Place} % (fold)
\label{sec:serieller_fall_vs_1d_mit_einem_place}
Jeder Algorithmus muss pro Iteration jeden aktiven Knoten mindestens einmal in irgendeiner Weise bearbeiten. Außerdem muss jeder Algorithmus pro Iteration jeden der Knoten, der von einem der aktiven Knoten aus erreichbar ist, mindestens einmal bearbeiten, . Der serielle Algorithmus tut genau das und nicht mehr. In Tests wurde herausgefunden, dass eine Iteration mittels einer herkömmlichen for-Schleife mit anschließendem direkten Elementzugriff über den Index deutlich schneller ist, als eine foreach-Schleife. Der 1D-Algorithmus muss pro Iteration die Knoten in Sendepuffer einsortieren (jeden aktiven Knoten einmal bearbeiten) und verschicken, was im Fall mit nur einem Place aber eine einfache Zeigerzuweisung ist. Anschließend muss dann nochmals jeder aktive Knoten betrachtet werden, um alle erreichbaren Knoten zu erhalten. Der 1D-Algorithmus muss also zweimal über alle aktiven Knoten iterieren, zumindest in einer naiven Implementierung. Die verwendete optimierte Version legt diese beide Phasen aber zusammen. Zusätzliche Arbeit, im Gegensatz zu der seriellen Version, hat der 1D Algorithmus also nur beim Zurückkopieren des gesamten BFS-Distanz-Arrays.

Die Messergebnisse sind in Abbildung \ref{fig:Vergleich_Seriell} illustriert. Sie zeigen, dass der serielle Algorithmus für dichte Graphen langsamer ist, als der 1D Algorithmus, wenn er mit nur einem Place gestartet wird. Nur bei dünn besetzten Graphen ist die Laufzeiten des seriellen Algorithmus ein Wenig schneller, wobei die Grenze bei den Tests bei einem durchschnittlichen Knotengrad von ca. 400 lag. Die Messergebnisse zur Variation der Größe (Kapitel \ref{sub:gr_e}) zeigen ebenfalls, dass auch bei großer Knotenzahl der serielle Algorithmus der schnellste ist, solange der durchschnittliche Knotengrad nicht zu groß wird.

Wieso der serielle Algorithmus auf Listenbasis nicht der schnellste ist, wie eigentlich erwartet, ist nicht ohne weiteres zu erklären. Zufall in Verbindung mit der Garbage Collection sind in Anbetracht der Deutlichkeit der Ergebnisse auszuschließen. Eine mögliche Erklärung ist, dass der Compiler den einen Code besser optimieren konnte als den anderen, ohne dass sofort offensichtlich ist, woran das liegt.

Wie ebenfalls aus Abbildung \ref{fig:Vergleich_Seriell} hervorgeht, ist der 2D Algorithmus deutlich langsamer als die beiden anderen. Der 2D Algorithmus hat zwei Kommunikationsphasen pro Iteration. In den beiden Phasen wir aber jeweils nur mit $\sqrt{p}$ anderen Places kommuniziert (bei p Places)\cite{Buluc:2011}, während im Fall des 1D Algorithmus jeder Place potentiell mit allen anderen kommuniziert. Im seriellen Fall ist diese zusätzliche Komplexität nicht nötig, verlangsamt aber den Ablauf.
% section serieller_fall_vs_1d_mit_einem_place (end)  

\begin{figure}
	\centering
	\label{fig:Vergleich_Seriell}
	\begin{tikzpicture}
	    \begin{axis}[
	       legend style={anchor=north west},
	        x tick label style={/pgf/number format/1000 sep= },
	        y tick label style={/pgf/number format/1000 sep= },
	        xlabel=Knotengrad,
	        ylabel=Zeit in ms]
	    \addplot[smooth,mark=*,blue] plot coordinates {
	        (5,29)
	        (50,56)
	        (100,79)
	        (250,151)
	        (500,265)
	        (700,355)
	        (800,400)
	        (900,450)
	        (1000,498)
	        (1200,575)
	        (1500,727)
	        (2000,947)
	    };
	    \addlegendentry{Seriell}

	    \addplot[smooth,color=red,mark=*]
	        plot coordinates {
	        (5,62)
	        (50,88)
	        (100,114)
	        (250,164)
	        (500,261)
	        (700,340)
	        (800,376)
	        (900,415)
	        (1000,451)
	        (1200,526)
	        (1500,643)
	        (2000,828)
	        };
	    \addlegendentry{1D}

	    \addplot[smooth,color=green,mark=*]
	        plot coordinates {
	        (5,85)
	        (50,155)
	        (100,234)
	        (250,444)
	        (500,808)
	        (700,1096)
	        (800,1239)
	        (900,1383)
	        (1000,1526)
	        (1200,1754)
	        (1500,2247)
	        (2000, 2969)
	        };
	    \addlegendentry{2D}
	    \end{axis}
	\end{tikzpicture}
	\caption{Vergleich der Laufzeiten bei serieller Ausführung, also nur ein Place bei 1D und 2D Algorithmus, jeweils schnellste gemessene Laufzeit.}
\end{figure}

\section{Ergebnisse der Parallelisierung} % (fold)
\label{sec:ergebnisse_der_parallelisierung}
Da die Breitensuche mit dem 2D Dekomposition keine vergleichbaren Ergebnisse lieferte, wird hier nur der serielle Algorithmus mit dem 1D Algorithmus verglichen. Mehr zu der 2D Breitensuche ist in Kapitel \ref{sec:die_2d_breitensuche} niedergeschrieben. Es wurden drei Testreihen durchgeführt.
\begin{description}
	\item[Dichte] Die Knotenzahl und die Verteilung sind konstant, die Kantenanzahl variiert. Die Knotenzahl ist 100 000, die Grenzen des Knotengrads sind konstant zwischen 1 und $\infty$
	\item[Verteilung] Die Knotenzahl und der durchschnittliche Knotengrad sind konstant, die Grenzen des Knotengrads variieren. Die Knotenzahl ist 100 000, der durchschnittliche Knotengrad 750
	\item[Größe] Der durchschnittliche Knotengrad und die Grenzen des Knotengrads sind konstant, die Knotenzahl variiert. Der durchschnittliche Knotengrad ist 100, der minimale Knotengrad 1, der maximale 500.
\end{description}
Diese Werte sind auch aus Tabelle \ref{tab:Testreihen} ersichtlich.

\begin{table}
\label{tab:Testreihen}
\begin{tabular}{p{3.7cm}|c|c|c}
  & Testreihe \enquote{Dichte} & Testreihe \enquote{Verteilung} & Testreihe \enquote{Größe} \\ \hline \hline
  Knotenzahl & 100 000 &  100 000 & variiert\\
  Knotengrad \o & variiert & 750 & 100\\
  Knotengrad & $\in [1,\infty[$ & variiert & $\in [1,500]$\\
 \end{tabular}
  \caption{Jede Testreihe besteht aus mehreren Graphen. Der Eintrag \enquote{variiert} bedeutet, dass diese Maßzahl zwischen den Graphen dieser Reihe variiert.}
\end{table}

\newpage
\input{evaluation_dichte.tex}
\input{evaluation_verteilung.tex}
\input{evaluation_groesse.tex}

% section ergebnisse_der_parallelisierung (end)

\section{Die 2D Breitensuche} % (fold)
\label{sec:die_2d_breitensuche}
Die 2D Breitensuche ist in jedem Testfall der mit Abstand langsamste Algorithmus gewesen, was so nicht zu erwarten war. Der implementierte Algorithmus ist zwar zunächst deutlich komplexer, als die anderen beiden, doch gibt es zwei theoretische Vorteile, die diesen Algorithmus studierenswert machen. Erstens sind die Gruppen von Places, die untereinander kommunizieren deutlich kleiner. In der ersten Kommunikationsphase sendet jeder Place seine Daten an eine ganze Spalte, und empfängt dafür von einer ganzen Zeile die Daten für den nächsten Schritt. Das sind $2 * \sqrt{p}$ Places insgesamt, mit denen jeder Place kommunizieren muss, also $O(p * 2 + \sqrt{p} * \frac{1}{2}) = O(p * \sqrt{p})= O(p^{\frac{3}{2}})$ Kommunikationsvorgänge im System. In der zweiten Kommunikationsphase muss jeweils nur eine Zeile miteinander kommunizieren. Das sind zusätzlich $O(\sqrt{p} * p * \frac{1}{2})$ Kommunikationsvorgänge. Es sind also im O-Kalkül pro Iteration $O(p^{\frac{3}{2}})$. Im Vergleich dazu kommuniziert jeder Place im 1D Algorithmus mit allen anderen $p - 1$ Places, also $O(p^2)$ Kommunikationsvorgänge im ganzen System pro Iteration. Der zweite Vorteil der Implementierung wie in \cite{Buluc:2011} ist die Möglichkeit, alle Kommunikation und Synchronisation auf drei MPI Operationen abzubilden, die auf entsprechender Hardware vergleichsweise schnell sind. 

Beide Vorteile sind bei der X10 Implementierung, die auf nur einem CPU läuft, hinfällig. Zunächst wurden die MPI Operationen in X10 \enquote{nachprogrammiert}. Um das zu tun, muss \textit{at} verwendet werden, dass einen für diesen Zweck unnützen Rückkanal bereithält, der zusätzliche Latenz in jede Kommunikation bringt. Ein Voteil von hochperformanter MPI Hardware liegt darin, dass ein Datum nur einmal gesendet werden muss, wenn es an mehrere Empfänger gehen soll. Das ist in X10-Syntax nicht möglich. Die zusätzliche Kommunikation ist also teurer, als sie sein müsste. 
Der zweite Vorteil, dass weniger Kommunikationsvorgänge im System stattfinden, ist aus zwei Gründen hier nicht ausschlaggebend. Zum einen sind so wenig Places im Spiel, dass es kaum einen Unterschied zwischen $O(p^{\frac{3}{2}})$ und $O(p^2)$ gibt, zum anderen haben erste Tests gezeigt, dass die Kommunikationsphasen etwa 4 bis 5 Mal so schnell sind wie die Rechenphasen, also die Kommunikation nicht so ausschlaggebend ist, wie das zu erwarten war. Das wiederum ist auf die Testumgebung ohne wirklich getrennte Places zurückzuführen.

Wie in \cite{Buluc:2011} ist also auch in X10 die 2D Implementierung die schlechtere, wobei in den Dimensionen, in denen in dieser Arbeit gedacht wird, die Unterschiede gravierender sind. Ob in größeren Testumgebungen die Ergebnisse anders ausfallen, muss in einer weiteren Arbeit untersucht werden.
% section die_2d_breitensuche (end)

\section{Invasive Breitensuche} % (fold)
\label{sec:invasive_breitensuche}
Im Rahmen dieser Arbeit wurden Ansätze herausgearbeitet, um die Breitensuche an das invasive Rechnen im Allgemeinen und an das Framework invadeX10 im Speziellen, anzupassen. Als Grundlage diente dazu der 1D-Algorithmus. Die invasive Version ist prinzipiell der selbe Algorithmus. Die zusätzliche Funktionalität und die zusätzlichen Schwierigkeiten, die in Kapitel \ref{sec:unterschiede_zum_nicht_invasiven_fall} beschrieben sind, generieren aber pro Iteration viel Overhead. Dadurch, dass der Kontrollfluss nach jeder Iteration wieder zur Masteractivity zurückkehrt, ist auch zusätzlicher Kommunikationsaufwand erforderlich. Die gemessenen Laufzeiten sind erwartungsgemäß  langsamer, als die der 1D-Breitensuche und aus den eben genannten Gründen nicht vergleichbar. Im Allgemeinen ist es unfair und vor allem nicht zielführend, die benötigte Zeit zur Lösung einer einzelnen BFS-Instanz zu vergleichen, da im invasiven Rechnen bewusst Nachteile bei der Lösung einer einzelnen Instanz in Kauf genommen werden, um eine Menge von Rechenjobs als Ganzes schneller bewältigt zu können. Die Intention des invasiven Rechnens ist nicht, eine einzelne Instanz möglichst schnell zu lösen. Um einen fairen Vergleich durchzuführen, muss eine Menge von verschiedenen Jobs und eine Zielhardware definiert werden. Daraufhin muss verglichen werden, wie lange es mit verschiedenen Strategien braucht, bis alle Jobs erledigt wurden. Diese Strategien könnten zum Beispiel sein, alle gleichzeitig starten, ein Job nach dem anderen starten oder invasives Vorgehen. In Mangel der invasiven Hardware und ausgereiften Softwareinstrumenten um diese zu simulieren, musste darauf in dieser Arbeit verzichtet werden. Die Ergebnisse sind aber insofern nicht schlecht, als dass die invasive Breitensuche in den Testläufen besser mit der Anzahl der Places skalierte, als die 2D-Breitensuche.  Für das beschriebene Testsetup ist der vorliegende Algorithmus somit ein guter Ausgangspunkt.
% section invasive_breitensuche (end)


% section wikipedia_und_die_philosopie (end)
% chapter ergebnisse_und_diskussion (end)