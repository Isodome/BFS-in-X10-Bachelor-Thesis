%!TEX root = thesis.tex

\chapter{Methoden} % (fold)
\label{cha:methoden}

\section{Graphen} % (fold)
\label{sec:graphen}
Die Breitensuche auf einem 1.2GB Beispielgraph läuft, je nach Algorithmus, in knapp einer Sekunde durch (Anhang \ref{Anhang-Messwerte-Dichte}). Wenn ein Algorithmus nur sehr kurz läuft, machen konstante Laufzeitschwankungen prozentual einen größeren Anteil aus. Wenn also beispielsweise ein Algorithmus durch das Betriebssystem, das gerade etwas andere machen muss, für 10ms unterbrochen wird, verfälscht das eine Messung die nur 50 ms läuft gravierender, als eine Messung, die mehrere Sekunden läuft. Deswegen wurde versucht, größtmögliche Beispielgraphen zu generieren. Für diese Arbeit wurden Graphen in der Größenordnung von 100 000 Knoten generiert. Diese wiederum relativ kleine Ausdehnung wurde gewählt, da der Testmodus unter anderem beinhaltet, dass bei gleichbleibender Knotenanzahl die Dichte des Graphen variiert. Sehr dicht besiedelte Graphen mit 100 000 Knoten passen gerade noch in den Arbeitsspeicher. Würden noch größere Graphen verwendet, müsste entweder die Dichte des Graphen beschränkt werden oder das Betriebssysteme müsste Speicherseiten auslagern. Da die Zeit für die Auslagerung mit in die Laufzeitmessung eingehen würde, ist zu erwarten, dass der Algorithmus dadurch deutlich langsam wäre, als bei Probleminstanzen, die in den Hauptspeicher passen. Die einzige Schlussfolgerung aus so einer Testreihe wäre, dass externe Algorithmen langsamer sind, als interne. 

Die asymptotische Laufzeit der Breitensuche ist O(n + m) ist \cite{SWB-283374373}. Das führt zu der Vermutung, dass bei gleicher (absoluter) Kantenzahl und erhöhter Knotenzahl, die Laufzeit nicht wesentlich länger ist. Ein Beispiel: BFS auf einem Graph mit 1 000 000 Knoten und durchschnittlichem Knotengrad von 100 (ergibt 100 Mio Kanten) sollte kaum länger dauern, als die BFS auf einem Graph mit 100 000 Knoten und durchschnittlichem Knotengrad von 1000 (ergibt ebenso 100 Mio Kanten). Diese Annahme ist für den parallelen Fall falsch, da die zu versendenden Datenmengen erheblich größer werden, wenn der Graph mehr Knoten hat.

Es wurde ein Tool namens graph-generator \cite{graph-generator:2009:Online} eingesetzt, um zufällige Graphen zu generieren. Um einen Graph zu erstellen, müssen folgende Parameter übergeben werden:
\begin{enumerate}
	\item Die Anzahl an Knoten.
	\item Der minimale Ausgangsgrad jedes Knotens.
	\item Der maximale Ausgangsgrad jedes Knotens.
	\item Der Exponent der Exponentialverteiltung.
	\item Der mittlerer Knotengrad z.
\end{enumerate}
Der Ausgangsgrad der Knoten ist folgendermaßen verteilt:
$$
P(X=k) \propto (k + offset)^{-exp}
$$
Der Offset wird dabei automatisch von dem Tool derart gewählt, dass sich ein durchschnittlicher Ausgangsgrad von z ergibt. Der Exponent der Exponentialverteilung \textit{exp} wurde vor den ersten Messungen auf 5 festgelegt und danach nicht mehr verändert. Die Zahl 5 wurde nach Tests mit verschiedenen Exponenten insofern als geeignet angesehen, als dass die Wahrscheinlichkeiten für Knoten mit sehr geringem Grad nicht zu hoch waren. Mit noch größeren Exponenten hatte der graph-generator Probleme.
% section graphen (end)

\section{Testplattform} % (fold)
\label{sec:testplattform}
Als Testplattform kann ein Apple Notebook von 2011 zum Einsatz. Es hat einen Intel Core i7-2720QM \enquote{Sandy Bridge} Prozessor, der mit 2.2Ghz getaktet wird. Es stehen 4 physikalische Kerne zur Verfügung, die jeweils Intels Hyper Threading Technologie unterstützen. Dadurch sind physikalisch 8 parallel laufende Threads möglich. Beim Vergleich von sequentiellen Algorithmen mit parallelen ist zu beachten, dass der Prozessor einen Kern auf bis zu 3.3 GHz übertakten kann, falls die anderen Kerne momentan nicht verwendet werden. Der optimal erreichbare Speedup ist demnach nicht 8.0, sondern deutlich darunter. Das Testsystem ist außerdem mit 8GB Hauptspeicher ausgestattet, der bei einem Takt von 1333Mhz arbeitet. Auf dem Testsystem wird als Betriebssystem Mac OS X 10.6.8 \enquote{Snow Leopard} und der x10 Compiler in der Version 2.2.3 verwendet.
% section testplattform (end)

\section{Modus} % (fold)
\label{sec:modus}
Um Ergebnisse aus je einer Algorithmus - Graph - Kombination zu erhalten, wird der gewählte Algorithmus drei mal auf dem gewählten Graph ausgeführt und die Zeit gemessen, die die Berechnung benötigt. Die Zeit, um den Graph in den Speicher einzulesen und die Daten auf die Places aufzuteilen, wurde nicht gemessen, da sie wenig mit dem Algorithmus oder X10 zu tun hat. Die Zeit, die benötigt wird, um das Ergebnis von den beteiligten Places zurück zum Ursprungsplace zu kopieren, wird allerdings mitgemessen.

Die X10 Laufzeitumgebung liest beim Programmstart die beiden Umgebungsvariablen X10\_NPLACES und X10\_NTHREADS, in denen steht, wie viele Places lokal auf diesem Rechner simuliert werden sollen und wie viele Threads jedem dieser Places zur Verfügung stehen sollen. Die einzelnen Places werden durch Prozesse (nicht Threads) repräsentiert, wodurch sie tatsächlich getrennte Speicherbereiche haben. Dieser Aufbau entspricht zwar nicht einem Setup, in dem jeder X10 Prozess auf einen physikalisch getrennten Computer operiert, doch auch der Kontextwechsel, der bei der Kommunikation zwischen Places auftritt, ist relativ langsam und somit eine Annäherung an realen Kommunikationsoverhead. Trotzdem sind diese Ergebnisse nicht eins zu eins auf einen Rechnerverbund zu übertragen. Das liegt zum einen daran, dass, die Kommunikation zwischen physikalisch getrennten Places nochmal erheblich teurer ist, als lokale Interprozesskommunkation, zum anderen können mit einem Rechnerverbund wesentlich größere Graphen bearbeitet werden, deren Effekte auf den Algorithmus in dieser Arbeit nicht gemessen werden.
Andererseits dürfen die Ergebnisse dieser Arbeit auch nicht mit der lokalen Parallelisierung der Breitensuche auf einem einzelnen Rechner verwechselt werden. Kommunikation mittels geteiltem Speicher ist deutlich schneller, als die hier verwendete Interprozesskommunikation. Es sei hier auch nochmal darauf hingewiesen, dass pro Prozess, also pro Place, immer nur ein Thread aktiv ist.
Um die Möglichkeit der Parallelität zu messen, wurde der Algorithmus in der 1D und der 2D Zerlegung jeweils in einer Konfiguration mit 1, 2, 4, 8 und 9 Places ausgeführt. Die Konfiguration mit 9 Places wurden hinzugenommen, da so bei der 2D Zerlegung eine symmetrische 3 mal 3 Zerlegung stattfinden kann. Es wurde vermutet, dass eine quadratische Anzahl an Places besonders günstig für diesen Algorithmus sind, obwohl 9 Places mehr sind, als Rechenkerne zur Verfügung stehen. Der Vollständigkeit halber wurde auch der 1D Algorithmus mit 9 Places durchgeführt. Um wirklich vollständige Ergebnisse zu bekommen, sollte eigentlich pro Graph die Breitensuche einmal von jedem Knoten aus gestartet werden.  Dieses Unterfangen würde den Zeitrahmen der Arbeit sprengen.

Der Modus, mit nur einem Thread pro Place und ohne die Nutzung von geteilten Speicher zwischen den einzelnen Ausführungsfäden lässt neben der Übertragbarkeit auf die invasive Hardware auch Rückschluss auf Manycore - Systeme zu. Bei diesen CPUs der Zukunft sollen viele kleine Rechenkerne auf einer CPU platziert werden. Cachekohärenz mit geteiltem Speicher und sehr vielen Kernen benötigt viel Overhead. Es werden deswegen auch Ansätze ohne geteilten Speicher verfolgt. Auch wird jeder Kern nur einen Ausführungsfaden gleichzeitig bearbeiten können. Die hier vorgeschlagenen Implementierungen sind also für solch eine Hardware interessant.
% section modus (end)

\chapter{Ergebnisse und Diskussion} % (fold)
\label{cha:ergebnisse_und_diskussion}

Die vollständigen Messergebnisse finden sich in den Anhängen \ref{Anhang-Messwerte-Dichte}, \ref{Anhang-Messwerte-Verteilung} und \ref{Anhang-Messwerte-Groesse}. Es wurden die Auswirkungen der Variation der Dichte, der Knotenzahl und der Verteilung des Graphen auf die Laufzeit gemessen, wobei jeweils die anderen Parameter fest gewählt waren. Sehr dünn besetzte und kleine Graphen waren deutlich schneller mit einer seriellen Version der Breitensuche zu lösen, als es mit jedwedem parallelen Algorithmus möglich war. Der Vergleich der seriellen Breitensuche, mit der 1D-Breitensuche mit einem Place ist durchaus auch interessant und wird in Kapitel \ref{sec:serieller_fall_vs_1d_mit_einem_place} vertieft. Der 2D-Algorithmus war in allen Testfällen der langsamste, weswegen er gesondert in Kapitel \ref{sec:die_2d_breitensuche} behandelt wird.

\section{Serieller Fall, 1D mit einem Place und 2D mit einem Place} % (fold)
\label{sec:serieller_fall_vs_1d_mit_einem_place}
Jeder Algorithmus muss pro Iteration jeden aktiven Knoten mindestens einmal in irgendeiner Weise bearbeiten. Außerdem muss jeder Algorithmus pro Iteration jeden der Knoten mindestens einmal bearbeiten, der von einem der aktiven Knoten aus erreichbar ist. Der serielle Algorithmus tut genau das und nicht mehr. In Tests wurde herausgefunden, dass eine Iteration mittels einer herkömmlichen for-Schleife mit anschließendem direkten Elementzugriff über den Index deutlich schneller ist, als eine foreach-Schleife. Der 1D-Algorithmus muss pro Iteration die Knoten in Sendepuffer einsortieren (jeden aktiven Knoten einmal bearbeiten) und verschicken, was im Fall mit nur einem Place aber eine einfache Zeigerzuweisung ist. Anschließend muss dann nochmals jeder aktive Knoten betrachtet werden, um alle erreichbaren Knoten zu erhalten. Der 1D-Algorithmus muss also zweimal über alle aktiven Knoten iterieren, zumindest in einer naiven Implementierung. Die verwendete optimierte Version legt diese beide Phasen aber zusammen. Zusätzliche Arbeit, im Gegensatz zu der seriellen Version, hat der 1D Algorithmus also nur beim Zurückkopieren des gesamten BFS-Distanz-Arrays.

Die Messergebnisse sind in Abbildung \ref{fig:Vergleich_Seriell} illustriert. Sie zeigen, dass der serielle Algorithmus für dichte Graphen langsamer ist, als der 1D Algorithmus, wenn er mit nur einem Place gestartet wird. Nur bei dünn besetzten Graphen ist die Laufzeiten des seriellen Algorithmus ein Wenig schneller, wobei die Grenze bei den Tests bei einem durchschnittlichen Knotengrad von ca. 400 lag. Die Messergebnisse zur Variation der Größe (Kapitel \ref{sub:gr_e}) zeigen ebenfalls, dass auch bei großer Knotenzahl der serielle Algorithmus der schnellste ist, solange der durchschnittliche Knotengrad nicht zu groß wird.

Wieso der serielle Algorithmus auf Listenbasis nicht der schnellste ist, wie eigentlich erwartet, ist nicht ohne weiteres zu erklären. Zufall in Verbindung mit der Garbage Collection sind in Anbetracht der Deutlichkeit der Ergebnisse auszuschließen. Eine mögliche Erklärung ist, dass der Compiler den einen Code besser optimieren konnte, als den anderen, ohne dass sofort offensichtlich ist, woran das liegt.

Wie ebenfalls aus Abbildung \ref{fig:Vergleich_Seriell} hervorgeht, ist der 2D Algorithmus deutlich langsamer als die beiden anderen. Der 2D Algorithmus hat zwei Kommunikationsphasen pro Iteration. In den beiden Phasen wir aber jeweils nur mit $\sqrt(p)$ anderen Places kommuniziert (bei p Places)\cite{Buluc:2011}, während im Fall des 1D Algorithmus jeder Place potentiell mit allen anderen kommuniziert. Im seriellen Fall ist diese zusätzliche Komplexität nicht nötig, verlangsamt aber den Ablauf.
% section serieller_fall_vs_1d_mit_einem_place (end)  

\begin{figure}
	\centering
	\label{fig:Vergleich_Seriell}
	\begin{tikzpicture}
	    \begin{axis}[
	        xlabel=Knotengrad,
	        ylabel=Zeit in ms]
	    \addplot[smooth,mark=*,blue] plot coordinates {
	        (5,29)
	        (50,56)
	        (100,79)
	        (250,151)
	        (500,265)
	        (700,355)
	        (800,400)
	        (900,450)
	        (1000,498)
	        (1200,575)
	        (1500,727)
	        (2000,947)
	    };
	    \addlegendentry{Seriell}

	    \addplot[smooth,color=red,mark=*]
	        plot coordinates {
	        (5,62)
	        (50,88)
	        (100,114)
	        (250,164)
	        (500,261)
	        (700,340)
	        (800,376)
	        (900,415)
	        (1000,451)
	        (1200,526)
	        (1500,643)
	        (2000,828)
	        };
	    \addlegendentry{1D}

	    \addplot[smooth,color=green,mark=*]
	        plot coordinates {
	        (5,85)
	        (50,155)
	        (100,234)
	        (250,444)
	        (500,808)
	        (700,1096)
	        (800,1239)
	        (900,1383)
	        (1000,1526)
	        (1200,1754)
	        (1500,2247)
	        (2000, 2969)
	        };
	    \addlegendentry{2D}
	    \end{axis}
	\end{tikzpicture}
	\caption{Vergleich der Laufzeiten bei serieller Ausführung, also nur ein Place bei 1D und 2D Algorithmus, jeweils schnellste gemessene Laufzeit.}
\end{figure}

\section{Ergebnisse der Parallelisierung} % (fold)
\label{sec:ergebnisse_der_parallelisierung}
Da die Breitensuche mit dem 2D Dekomposition keine vergleichbaren Ergebnisse lieferte, wird hier nur der serielle Algorithmus mit dem 1D Algorithmus verglichen. Mehr zu der 2D Breitensuche ist in Kapitel \ref{sec:die_2d_breitensuche} niedergeschrieben. Es wurden drei Testreihen durchgeführt.
\begin{description}
	\item[Dichte] Der Knotenzahl und die Verteilung sind konstant, die Kantenanzahl variiert. \newline Die Knotenzahl ist 100 000, die Grenzen des Knotengrads sind konstant zwischen 1 und $\infty$
	\item[Verteilung] Die Knotenzahl und Kantenanzahl sind konstant, die Grenzen des Knotengrads variieren.\newline Die Knotenzahl ist 100 000, der Knotengrad 750
	\item[Größe] Der durchschnittliche Knotengrad und die Grenzen des Knotengrads sind konstant, die Knotenzahl variiert. Der durchschnittliche Knotengrad ist 100, der minimale Knotengrad 1, der maximale 500.
\end{description}
Diese Werte sind auch aus Tabelle \ref{tab:Testreihen} ersichtlich.

\begin{table}
\label{tab:Testreihen}
\begin{tabular}{p{3.7cm}|c|c|c}
  & Testreihe \enquote{Dichte} & Testreihe \enquote{Verteilung} & Testreihe \enquote{Größe} \\ \hline \hline
  Knotenzahl & konstant 100 000 & konstant 100 000 & variiert\\
  Kantengrad \o & variiert & konstant 750 & konstant 100\\
  Kantengrad Grenzen & konstant $\ge$1, $<\infty$ & variiert & konstant $\ge$1, $\le$500\\
 \end{tabular}
  \caption{Jede Testreihe besteht aus mehreren Graphen. Der Eintrag \enquote{variiert} bedeutet, dass diese Maßzahl zwischen den Graphen dieser Reihe variiert. Im Gegensatz dazu bedeutet \enquote{konstant}, dass diese Maßzahl für alle Graphen dieser Reihe gilt.}
\end{table}

\newpage
\input{evaluation_dichte.tex}
\input{evaluation_verteilung.tex}
\input{evaluation_groesse.tex}

% section ergebnisse_der_parallelisierung (end)

\section{Die 2D Breitensuche} % (fold)
\label{sec:die_2d_breitensuche}
Die 2D Breitensuche ist in jedem Testfall der mit Abstand langsamste Algorithmus gewesen, was so nicht zu erwarten war. Der implementierte Algorithmus ist zwar zunächst deutlich komplexer, als die anderen beiden, doch gibt es zwei theoretische Vorteile, die diesen Algorithmus studierenswert machen. Erstens sind die Gruppen von Places, die untereinander kommunizieren deutlich kleiner. In der ersten Kommunikationsphase sendet jeder Place seine Daten an eine ganze Spalte, und empfängt dafür von einer ganzen Zeile die Daten für den nächsten Schritt. Das sind $2 * \sqrt(p)$ Places, insgesamt, mit denen jeder Place kommunizieren muss, also $O(p * 2 + \sqrt(p) * \frac{1}{2}) = O(p * \sqrt(p))= O(p^{\frac{3}{2}})$ Kommunikationsvorgänge im System. In der zweiten Kommunikationsphase muss jeweils nur eine Zeile miteinander kommunizieren. Das sind zusätzlich $O(\sqrt(p) * p * \frac{1}{2})$ Kommunikationsvorgänge. Es sind also im O-Kalkül pro Iteration $O(p^{\frac{3}{2}})$. Im Vergleich dazu kommuniziert jeder Place im 1D Algorithmus mit allen anderen p-1 Places, also $O(p^2)$ Kommunikationsvorgänge. Der zweite Vorteil der Implementierung wie in \cite{Buluc:2011} ist die Möglichkeit, alle Kommunikation und Synchronisation auf drei MPI Operationen abzubilden. Die auf entsprechender Hardware vergleichsweise schnell sind. 

Beide dieser Vorteile sind bei der X10 Implementierung, die auf nur einem CPU läuft, hinfällig. Zunächst wurden die MPI Operationen in X10 \enquote{nachprogrammiert}. Um das zu tun, muss \textit{at} verwendet werden, dass einen für diesen Zweck unnützen Rückkanal bereithält, der zusätzliche Latenz in jede Kommunikation bringt. Weiterhin gibt es bei der Verwendung einer hochperformanten MPI Hardware den Vorteil, dass ein Datum nur einmal gesendet werden muss, wenn es an mehrere Empfänger gehen soll. Das ist in X10-Syntax nicht möglich. Die zusätzliche Kommunikation ist also teurer, als sie sein müsste. 
Der zweite Vorteil, eben dass weniger Kommunikationsvorgänge im System sind, ist aus zwei Gründen hier nicht ausschlaggebend. Zum einen sind dermaßen wenig Places im Spiel, dass es kaum einen Unterschied zwischen $O(p^{\frac{3}{2}})$ und $O(p^2)$ gibt, zum anderen haben erste Tests gezeigt, dass die Kommunikationsphasen etwa 4 bis 5 Mal so schnell sind wie die Rechenphasen, also die Kommunikation nicht so ausschlaggebend ist, wie das zu erwarten war. Das wiederum ist auf die Testumgebung ohne wirklich getrennte Places zurückzuführen.

Wie in \cite{Buluc:2011} ist also auch in X10 die 2D Implementierung eher die schlechtere, wobei in den Dimensionen, in denen in dieser Arbeit gedacht wird, die Unterschiede gravierender sind. Ob in größeren Testumgebungen die Ergebnisse anders ausfallen, muss in einer weiteren Arbeit untersucht werden.
% section die_2d_breitensuche (end)

\section{Invasive Breitensuche} % (fold)
\label{sec:invasive_breitensuche}
Im Rahmen dieser Arbeit wurden Ansätze herausgearbeitet, um die Breitensuche an das invasive Rechnen im Allgemeinen und an das Framework invadeX10 im Speziellen, anzupassen. Als Grundlage diente dazu der 1D-Algorithmus. Die invasive Version ist prinzipiell der selbe Algorithmus. Die zusätzliche Funktionalität und die zusätzlichen Schwierigkeiten, die in Kapitel \ref{sec:unterschiede_zum_nicht_invasiven_fall} beschrieben sind, generieren aber pro Iteration viel Overhead. Dadurch, dass der Kontrollfluss nach jeder Iteration wieder zur Masteractivity zurückkehrt, ist auch zusätzliche Kommunikationsaufwand notwendig. Die gemessenen Laufzeiten sind erwartungsgemäß  langsamer, als die der 1D-Breitensuche und aus den eben genannten Gründen nicht vergleichbar. Allgemein ist es unfair und vor allem nicht zielführend, die benötigte Zeit zur Lösung einer einzelnen BFS-Instanz zu vergleichen, da im invasiven Rechnen bewusst Nachteile bei der Lösung einer einzelnen Instanz in Kauf genommen werden, um eine Menge von Rechenjobs als Ganzes schneller bewältigt zu können. Die Intention des invasiven Rechnens ist nicht, eine einzelne Instanz möglichst schnell zu lösen. Um einen fairen Vergleich durchzuführen, muss eine Menge von verschiedenen Jobs und eine Zielhardware definiert werden. Daraufhin muss verglichen werden, wie lange es mit verschiedenen Strategien braucht, bis alle Jobs erledigt wurden. Die Strategien sind zum Beispiel, alle gleichzeitig starten, ein Job nach dem anderen starten oder eben invasives Vorgehen. In Mangel der invasiven Hardware und ausgereiften Softwareinstrumenten um diese zu simulieren, musste darauf in dieser Arbeit verzichtet werden. Die Ergebnisse sind aber insofern nicht schlecht, als dass die invasive Breitensuche in den Testläufen besser mit der Anzahl der Places skalierte, als die 2D-Breitensuche.  Für das beschriebene Testsetup ist der vorliegende Algorithmus somit ein guter Ausgangspunkt.
% section invasive_breitensuche (end)


% section wikipedia_und_die_philosopie (end)
% chapter ergebnisse_und_diskussion (end)